(window.webpackJsonp=window.webpackJsonp||[]).push([[68],{420:function(s,r,t){"use strict";t.r(r);var a=t(25),e=Object(a.a)({},(function(){var s=this,r=s.$createElement,t=s._self._c||r;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h4",{attrs:{id:"turms集群间的架构设计"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#turms集群间的架构设计"}},[s._v("#")]),s._v(" Turms集群间的架构设计")]),s._v(" "),t("p",[s._v("Turms内部集群实现在早期内部开发阶段偏纯技术型的本质是根据Turms的即时通讯场景定制的轻量级分布式内存集群。")]),s._v(" "),t("p",[s._v("Turms的内部架构设计有一点很取巧，Turms即没采用诸如Consu、Zookeeper、Eureka等服务注册中心或配置中心方案，也不是采用设。Turms通过MongoDB实现发布订阅模型")]),s._v(" "),t("p",[s._v("简单来说，Turms内部架构实现是比较清晰的，具体技术与实现如下：")]),s._v(" "),t("ol",[t("li",[s._v("服务注册与发现。①首先查找种子节点。分为三大类：指定种子节点、通过Multicast轮询查找其他节点（Netty）、通过MongoDB充当服务注册中心通过监听服务记录增改删操作来寻找其他节点（Netty）；②同时当有新节点加入，则以最后一个节点加入时间为准，间隔3秒通过Gossip协议共享节点；③过滤节点（如对比节点版本、集群配置等）。④只要当前节点以连接的节点数满足脑裂quorum数，且3秒内没有新节点加入，且没收到VoteRequest，且判断本地优先级最高，则进入发起选举请求（无需管其他节点的连接节点的情况），⑤若获得Math.max(已知节点数, quorum)/2数量的vote则发送，则成为Master节点，并通知其他节点（带有当前集群hash信息与成员列表最后更新时间+纪元信息）④待集群状态达成一致时，进入选举阶段。")]),s._v(" "),t("li",[s._v("选举。通过Bully算法实现按照“优先级”/“权重”选举出Master节点，这也是没采用Raft算法选举的原因（补充：目前Turms集群需要Master负责的事情，对性能消耗很小）。Basic Paxos有活锁问题，实现麻烦且两次RPC调用效率低")]),s._v(" "),t("li",[s._v("在选举过程中，不接受新节点的加入请求。但注意每届选举，每个节点只能投一票。（如果出现同一时刻，多个“我是Master”的通知，则以时间戳最早的为Master，如果时间戳相同，则以ID最小的为主）")]),s._v(" "),t("li",[s._v("若选举失败，则间隔15秒再次发起选举")]),s._v(" "),t("li",[s._v("若一个节点发现有多个节点同时声明其为Master（如实际部署4个节点，并quorum设为2，则可能出现两个Master，同时其中一个节点可以同时连着两个Master），则该节点会连上一个Master，并告知其出现脑裂问题，让Master与其他Master再次选举。则Master之间需要通过Bully算法再次选择唯一的Master。")]),s._v(" "),t("li",[s._v("当一个Master")]),s._v(" "),t("li",[s._v("之后若有涉及状态同步的操作，则分为两种：1.只需要Master批准就算操作成功（注意此时有Master切换风险）；2.需要所有节点批准才算操作成功")])]),s._v(" "),t("h5",{attrs:{id:"为什么移除hazelcast"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#为什么移除hazelcast"}},[s._v("#")]),s._v(" 为什么移除Hazelcast")]),s._v(" "),t("p",[s._v("从历史角度来看，Hazelcast代码尾大不掉，底层，为求稳定。")]),s._v(" "),t("p",[s._v("Turms早期采用Hazelcast作为集群状态维护与数据交互的实现是因为Hazelcast时至今日仍是Java领域里最为优秀且通用的分布式内存实现的技术框架（Apache Ignite面向大数据场景，通用性不如Hazelcast）。但由于Hazelcast团队对集群间的性能问题不是很重视，对于许多很明显且严重影响性能的问题，要么视而不见，要么将解决方案一推再推，更别提背压与熔断等高级特性，这对于专注于分布式内存计算的技术框架来说是很不应该的。")]),s._v(" "),t("p",[s._v("线程方面，Hazelcast依靠阻塞的线程池机制，来实现伪异步调用，一些对外暴露为异步的操作其内部通过同步的乐观锁或synchronize重量锁，等待RPC结果的回调，效率极低，与NIO理念背道而驰。且胡乱使用线程池，尤其是胡乱使用Cached Executors，使得线程数很容易高达上百个。")]),s._v(" "),t("p",[s._v("作为一个发展多年的分布式内存技术解决方案，甚至不支持堆外内存发送Task请求（RPC），使得RPC的实现性能更低，也没有充分考虑缓存伪共享与堆内堆外内存等问题。并且由于代码尾大不掉，就算经过了4.0版本的大调整，代码仍然如此冗余（如Multicast就有SPI和两个重复的实现。但值得承认的是Hazelcast的接口）。")]),s._v(" "),t("p",[s._v("原计划是在Hazelcast的基础上加以重构，以满足Turms的需求。但由于其冗余的历史遗留代码过多且底层技术实现过于老套，积重难返，因此Turms后期基于RSocket协议与reactor-netty技术架构，自行重写了一套为Turms量身定做的现代且轻量的高性能集群解决方案。当然，Hazelcast的优点也很多，开箱即用，提供了大量使用简单又实用的各种分布式操作（包括分布式锁/事务/CP分区等高级特性）。并且其整体的架构设计也具有很好的参考意义，将其代码作为分布式内存实现的必读代码也不过分，Turms自行实现的集群架构就参考了Hazelcast的架构设计（Turms的集群架构实现主要参考：Hazelcast/Ignite/Elasticsearch/gRPC与传统微服务架构）。")]),s._v(" "),t("p",[s._v("至于其他的一些技术框架，如Ignite，虽然它代码写的很漂亮，但是它很明显是为专门的数据处理服务端设计的，并且太重了，拿给Turms用就有些杀鸡用牛刀了。另一方面，gRPC或其他RPC框架，更面向异构微服务设计，采用通用的Protobuf作为数据传输格式，但其实空间效率与CPU效率大不如定制的序列化来得高。而且层级太高，不方便后期优化，尤其是HTTP2的全双工比较鸡肋。")]),s._v(" "),t("p",[s._v("Turms服务端内部也有一个“拉闸”机制，")]),s._v(" "),t("p",[s._v("Turms集群间的架构比较清晰，Turms的所有跨服务器调用也都是异步的。")]),s._v(" "),t("p",[s._v("（特别补充：Turms的内部集群实现不提供供开发者实现的SPI接口或插件机制，也没有计划去提供）")]),s._v(" "),t("p",[s._v("首先判断quorum数量是否满足要求，且已完成集群的配置同步，如果是则对外提供服务，不是则不提供服务。")]),s._v(" "),t("p",[s._v("每个Turms服务端程序都有一个Node实例，该实例在启动时会触发搜寻其他Turms服务端IP的Discovery线程（Multicast的实现是Netty的单线程NioDatagramChannel，）。")]),s._v(" "),t("p",[s._v("每当发现新的服务端IP时（持续监听），当前Node实例都会与其尝试建立TCP连接（基于RSocket协议与reactor-netty）。Turms的选举算法很简单，就是基于Bully算法思想比较某个值的大小。默认实现是以最早加入集群的Node为Master，后期还将提供其他算法变种供开发者选择。")]),s._v(" "),t("p",[s._v("其他")]),s._v(" "),t("p",[s._v("0.Turms的SnowflakeID算法是参考Elasticsearch的实现适配而来（补充：Elasticsearch采用15bytes作为ID），Turms使用传统的64bits的long作为实现，并且其中的sequenceNumber占11bits（2,048）这个在大部分情况下都是绰绰有余的。为了更好地优化，Turms还Turms内部默认使用16个ID生成器实例，不同Turms业务服务使用不同的ID生成器。一方面减少线程竞争，减少乐观的CAS带来的，另一方面最终实现效果类似于给sequenceNumber加了4个bits。并且由于Turms整个技术方案都是异步的，因此线程数量会远少于传统的同规模大小的服务端实现，因此更有利于SnowFlakeID。")]),s._v(" "),t("p",[s._v("同时由于Turms的序列化与反序列化操作都是定制实现的（对于有必要且频繁的操作都是按位来定制），且针对使用目的直接进行堆内或堆外内存分配，以实现“零拷贝”，因此其性能与会比Protobuf实现还好，占用内存也更少。")]),s._v(" "),t("p",[s._v("并且Turms的内部集群实现中，有一个专门的IrresponsibleUserMap，其实现比较特殊，是专门为Turms的场景专门定制的。其实现类似于Redis的主从复制机制全量同步与增量同步")]),s._v(" "),t("p",[s._v("RPO")]),s._v(" "),t("h4",{attrs:{id:"集群中心化与去中心化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#集群中心化与去中心化"}},[s._v("#")]),s._v(" 集群中心化与去中心化")]),s._v(" "),t("p",[s._v("Turms的集群实现是中心化，即带Leader节点。Turms的共识算法是定制的，主要参考Raft共识算法与Bully选举算法，具体代码实现借鉴Atomix/Raft，因为它的代码很漂亮。Turms没有直接采用具体的Raft实现是因为，首先，一方面单纯地实现Raft算法架构后再引入Turms架构实现，代码显得太呆板，得额外写一层胶水代码，另一方面如果在Raft实现中嵌入Turms业务代码，代码又显得混乱，也不利于维护。其次，Raft自身职能跨度太大，不利于Turms的功能解耦。再次，杀鸡用牛刀，Turms不需要完整的Raft功能，就算引入也需要做很多减法。因此Turms借鉴其思想自行实现。")]),s._v(" "),t("p",[s._v("具体而言，Turms在集群尚未成型时需要对成员IP进行统一的收集。")]),s._v(" "),t("p",[s._v("考虑到Turms集群有稳定的Leader角色，因此Turms并为采用去中心化的经典实现，如Gossip、Basic Paxos。")]),s._v(" "),t("p",[s._v("各种选举算法其思想其实也都是相似的，Turms的集群算法主要参考了Elasticsearch的Bully算法与ZooKeeper的ZAB算法。")]),s._v(" "),t("p",[s._v("如果用户配错了quorum值，则集群可能出现多个Leader")]),s._v(" "),t("p",[s._v("关于脑裂")]),s._v(" "),t("p",[s._v("如果Turms集群出现脑裂问题，则Admin API不可用，对于客户端则用户状态不一致，但是仍可操作")])])}),[],!1,null,null,null);r.default=e.exports}}]);