"use strict";(self.webpackChunkturms_docs=self.webpackChunkturms_docs||[]).push([[502],{1081:(e,r,d)=>{d.r(r),d.d(r,{default:()=>o});var t=d(6252);const a=(0,t.uE)('<h1 id="状态感知" tabindex="-1"><a class="header-anchor" href="#状态感知" aria-hidden="true">#</a> 状态感知</h1><p>状态感知分为两大类，一类是<code>用户在线状态感知</code>，另一类是<code>业务模型状态感知</code>（如收到新消息、群成员发送变化）。</p><p>由于状态感知的具体实现与具体的产品需求有着密切关系，因此需要您能够把握住以下两点：</p><ol><li>判断产品需求是否合理。通常不合理的需求，诸如：一个群内可以有10000名用户，当一名用户发送消息时，要保证这条消息能100%地传送给其他9999名用户，并且用户能够拉取几年前的聊天信息。</li><li>分清主次需求，尽可能在质量属性之间取得平衡。IM服务的实现细节繁多，是否真的有必要为了兼容极端情况，而设计大量的兜底策略（如消息会话级自增ID），既大幅度地增加了开发成本与故障点，也让服务端总体的吞吐量下降。</li></ol><h2 id="用户在线状态感知" tabindex="-1"><a class="header-anchor" href="#用户在线状态感知" aria-hidden="true">#</a> 用户在线状态感知</h2>',5),i=(0,t.Uk)("简而言之，Turms通过心跳包来检测用户TCP连接的健康状态并以此判断用户是否“在线”。另外，如果您不关心底层实现，您仅需阅读："),l={href:"https://turms-im.github.io/docs/for-developers/client-api.html#%E4%BC%9A%E8%AF%9D%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F",target:"_blank",rel:"noopener noreferrer"},h=(0,t.Uk)("客户端API——会话的生命周期"),s=(0,t.Uk)("。"),n=(0,t.uE)('<h3 id="具体原理-拓展知识" tabindex="-1"><a class="header-anchor" href="#具体原理-拓展知识" aria-hidden="true">#</a> 具体原理（拓展知识）</h3><h4 id="背景" tabindex="-1"><a class="header-anchor" href="#背景" aria-hidden="true">#</a> 背景</h4><p>从网络传输层来看，TCP只是一个虚拟的连接，需要通过双向的消息传递与消息确认来模拟物理连接，因此如果客户端与服务端之间的连接实际上断开了，但在没有完成四次挥手（即没有完成指定的消息传递与确认）的情况下，TCP仍然判定该连接属于保持状态（如果此时试图从该TCP连接中读取数据，则会抛出带有类似于“An existing connection was forcibly closed by the remote host”消息的异常）。因此对于基于TCP协议开发的上层即时通讯应用而言，如果我们不做额外的工作，服务端就只能错误认为“该用户处于在线状态”。</p><h4 id="tcp没完成四次挥手的常见原因" tabindex="-1"><a class="header-anchor" href="#tcp没完成四次挥手的常见原因" aria-hidden="true">#</a> TCP没完成四次挥手的常见原因</h4><ul><li>客户端：客户端应用被强制关闭</li><li>服务端：负载持续过高无法响应；服务器直接宕机，导致服务端应用被强制关闭</li><li>链路中间路由：意外中断（如：移动接入网NAT超时）</li></ul><h4 id="应对异常断开连接的方案" tabindex="-1"><a class="header-anchor" href="#应对异常断开连接的方案" aria-hidden="true">#</a> 应对异常断开连接的方案</h4><p>为了保证服务端能感知到“用户下线”了的状态，Turms客户端会在上一个任意类型请求（如发送消息请求）的定长时间间隔后（暂不支持根据网络状况配置智能心跳），向服务端发送心跳包来维护其“在线状态”。服务端在收到客户端发来的心跳包或者其他业务请求后，都会在Redis服务端处刷新客户端的在线状态，以此来保活。</p><h2 id="业务模型状态感知" tabindex="-1"><a class="header-anchor" href="#业务模型状态感知" aria-hidden="true">#</a> 业务模型状态感知</h2><p>为了让用户能感知业务模型状态的变化（增删改），Turms支持推模式（服务端主动通知）、拉模式（客户端主动拉取机制。支持按Timeline拉取）以及推拉结合模式，以在实时性与资源消耗之间取得平衡，并让开发者能够自行调整实时性与资源消耗之间的权重。</p><h3 id="感知方式" tabindex="-1"><a class="header-anchor" href="#感知方式" aria-hidden="true">#</a> 感知方式</h3><h4 id="方式一-推模式-服务端主动通知" tabindex="-1"><a class="header-anchor" href="#方式一-推模式-服务端主动通知" aria-hidden="true">#</a> 方式一：推模式（服务端主动通知）</h4><p>推模式指的是：当某个业务模型发生变化时（由于增删改操作），服务端将主动通知相关在线用户该事件的发生。而当客户端收到通知时，Turms客户端会触发<code>NotificationService</code>中的<code>onNotification</code>回调函数。该函数的参数为<code>TurmsRequest</code>对象，表明触发该事件的请求。</p><p>通知相关行为可以根据：<code>im.turms.server.common.property.env.service.business.NotificationProperties</code>类进行配置。每一种通知类型都可以单独配置，并且所有通知相关配置均可在集群运行时进行动态更新。</p><h5 id="示例" tabindex="-1"><a class="header-anchor" href="#示例" aria-hidden="true">#</a> 示例</h5><p>以<code>im.turms.server.common.property.env.service.business.NotificationProperties#notifyMembersAfterGroupUpdated</code>这个属性为例。该属性用于控制“当群组信息发生变化时，是否通知群组成员”。这里的群组信息指的是：群组名称、群组类型、群组禁言时间等这样具有全局性的群组信息。</p><p>如果您将该属性值设置为true，则当群组信息发生变化时，群组成员的客户端都将收到触发该变化的通知。否则，群组成员客户端不会收到任何通知。</p><h5 id="评价" tabindex="-1"><a class="header-anchor" href="#评价" aria-hidden="true">#</a> 评价</h5><p>通知机制可以保证通知能实时地传递给相关用户，但其缺点就在于它很容易导致无意义的资源消耗（以具体业务场景为准）。比如用户A已经加入了100个群组，但该用户平时只查看其中3个群组的信息。这种场景下，如果100个群组的所有状态变化都开启了通知机制，则不管是服务端还是客户端都需要浪费大量资源去处理这些无意义的通知（因为该用户从来不看这些通知）。</p><p>为了解决该类问题，以及满足其他常见需求（如：要求当时离线的用户在上线时也能检测到业务模型是否发生变化；要求在线用户在通知被关闭的情况下也能感知业务模型的变化），Turms还提供了拉模式（客户端主动拉取）让用户来感知业务模型的变化。</p><h4 id="方式二-拉模式-客户端主动拉取。支持按timeline拉取" tabindex="-1"><a class="header-anchor" href="#方式二-拉模式-客户端主动拉取。支持按timeline拉取" aria-hidden="true">#</a> 方式二：拉模式（客户端主动拉取。支持按Timeline拉取）</h4><p>为了弥补上述提到的推模式的不足，Turms还提供了拉模式。</p><h5 id="大概实现" tabindex="-1"><a class="header-anchor" href="#大概实现" aria-hidden="true">#</a> 大概实现</h5><p>Turms的每个业务模型都带有一个版本信息，这个版本信息记录了该业务模型最后一次更新的时间。当客户端向服务端请求资源时，可以携带客户端最后一次更新该业务模型的时间（也可以不带），Turms服务端会对这个版本信息与当前业务模型的版本信息进行比对，如果客户端发来的版本信息早于当前业务模型的版本信息，则Turms服务端会返回最新的业务模型数据，否则抛出状态码<code>NO_CONTENT</code>，在客户端处则会收到空数据。</p><h5 id="常见拉取时机-同步时机" tabindex="-1"><a class="header-anchor" href="#常见拉取时机-同步时机" aria-hidden="true">#</a> 常见拉取时机（同步时机）</h5><ul><li>当您的应用被切换到前台时</li><li>会话重新连接上时</li><li>根据具体业务而定（看下文示例）</li></ul><h5 id="示例-1" tabindex="-1"><a class="header-anchor" href="#示例-1" aria-hidden="true">#</a> 示例</h5><p>继续以上述的案例为例。假设我们希望群组成员之间能够实时感知其他群组成员资料信息的变化。那如果我们采用通知机制，假设每个群除了用户A还有其他100名在线用户，则用户A的资料信息变化，需要向其他10000（100群*100人/群）名群组成员发送通知，这在实际运用中是绝对不可取的。</p><p>在实际运用中，通常会在特定时机（比如在用户打开某名用户的个人信息UI界面时，或者打开和某人的聊天窗口时），才让客户端主动请求服务端该用户的信息。同时，通过版本对比，减少无意义的资源浪费。</p><p>这种时刻注意实时性与资源消耗的设计要牢记在心中，以免设计出不切实际的应用场景。</p><h3 id="客户端对用户行为感知的实时性与服务端延迟" tabindex="-1"><a class="header-anchor" href="#客户端对用户行为感知的实时性与服务端延迟" aria-hidden="true">#</a> 客户端对用户行为感知的实时性与服务端延迟</h3><p>以拉黑用户的相关实现为例，Turms默认对用户关系进行1分钟的缓存，以避免频发查询数据库，这是合理的行为。如果此时用户A“拉黑”了用户B，那么可能会出现：虽然用户A拉黑了用户B，但在有缓存的这段时间里，用户B仍然有可能可以给用户A发送消息（因为Turms服务端是分布式集群，关系缓存与接收拉黑请求操作的服务端不一定是同一个服务端）。<strong>这种行为对Turms服务端是可以接受的，而不是Bug</strong>。</p><p>其合理且理想的参考解决方案是：在客户端的业务层面上（业务逻辑由您控制，而非由Turms客户端控制），就算Turms服务端发送给Turms客户端消息，您的客户端也应该根据您产品自身的业务逻辑，再做一次是否已拉黑用户判断，如果是，则隐藏不显示。</p><h2 id="消息感知" tabindex="-1"><a class="header-anchor" href="#消息感知" aria-hidden="true">#</a> 消息感知</h2><h3 id="读扩散与写扩散" tabindex="-1"><a class="header-anchor" href="#读扩散与写扩散" aria-hidden="true">#</a> 读扩散与写扩散</h3><p>总的来说，Turms架构基于读扩散消息模型设计，未来可能会辅之以写扩散做配合（即私聊与小群用写扩散，大群用读扩散）。</p><table><thead><tr><th></th><th>读扩散</th><th>写扩散</th></tr></thead><tbody><tr><td>含义</td><td>1. 每名用户跟与其聊天的其他用户或群都有一个独立会话（也叫做信箱或Timeline）。<br>2. 当用户发送消息时，只需要对消息进行一次写操作（操作数与会话内成员数无关）。<br>3. 当用户查询消息时，需要向服务端发送请求来拉取每一个会话的消息</td><td>1. 每名用户有且仅有一个信箱。<br>2. 当用户发送消息时，需要把这条消息写到该会话内的所有成员信箱里。<br>3. 当用户查询消息时，只需要从自己的信箱里读取消息</td></tr><tr><td>优势场景</td><td>用户会话（私聊会话与群里会话）相对少，群人数多的场景</td><td>适合私聊会话相对多，但群会话少且群人数也少的场景</td></tr><tr><td>劣势场景</td><td>用户的会话数量，且读的频繁</td><td>单个群的用户数多，且发送消息频发</td></tr><tr><td>技术实现</td><td>可以通过副本架构，对读请求进行负载均衡</td><td>1. 写操作难以进行负载均衡<br>2. 更新消息、撤回消息等功能实现成本巨大（分布式一致性问题与消息风暴）</td></tr><tr><td>消息可靠性</td><td>如产品对消息可靠性有较高的要求（保证消息不丢，保证消息内容一致），读扩散对应的实现相对简单，且性能比写扩散好得多</td><td>为保证消息写入到每位群成员的信箱中，需要引入弱分布式一致性事务（或强分布式一致性事务），性能低下</td></tr><tr><td>总评</td><td>适用的产品面广。对比读扩散与写扩散各自的劣势场景，读扩散的劣势场景依然能保证较高的效率</td><td></td></tr></tbody></table><p>提醒：“撤回消息”在实现上也是一条消息（一种特殊的系统消息）。</p><h3 id="消息接收、消息更新与消息撤回" tabindex="-1"><a class="header-anchor" href="#消息接收、消息更新与消息撤回" aria-hidden="true">#</a> 消息接收、消息更新与消息撤回</h3><p>Turms基于上述的“推模式”与“拉模式”实现客户端的消息接收、更新与撤回。其中：</p><ul><li>结合上述的<code>常见拉取时机</code>与下文的<code>关于消息的可达性、有序性与重复性</code>，Turms是可以实现100%消息必达、消息一致性排序与去重</li><li>消息更新与撤回的通知本质上也是一条消息，即一条特别的系统消息。Turms在接收到用户发出的消息更新或撤销请求后，会先判断该功能是否启动、用户是否有权限、是否在一定时间区间内等等判断，如果验证通过，则会先对存储在数据库的目标原消息记录做修改，然后再像发送其他普通消息一样，通过一条自动生成的系统消息，告知客户端之前某条消息已被更新或撤回，请自行做对应业务上的处理。</li></ul><h3 id="关于消息的可达性、有序性与重复性" tabindex="-1"><a class="header-anchor" href="#关于消息的可达性、有序性与重复性" aria-hidden="true">#</a> 关于消息的可达性、有序性与重复性</h3><p>架构设计永远是平衡的艺术，盲目承诺消息100%必达只是一种销售的说辞。好比大部分互联网应用在分布式事务的技术实现上，只会采用性能更好的弱分布式事务，而非虽然更可靠但性能低下的强分布式事务。是否需要实现100%的消息必达还是需要根据业务场景而定。如在直播聊天室场景，不仅不要求消息必达，甚至还会要求服务端要能根据负载情况与消息优先级，主动丢弃用户消息，或者只将消息发送给一部分用户。</p><p>直播场景也可能不强制要求消息有序性，而是要求“怎样消息吞吐量大，怎样设计。尽量保证消息的有序性，但不提供额外辅助资源进行支持”。</p><p>因此再次强调：做功能设计时，要分清主次需求，尽可能在质量属性之间取得平衡。切忌脱离业务场景，闭门造车。</p><h4 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h4><p>由于下文各种消息特性的具体实现对比相对复杂，该总结部分为您快速归纳最终方案。</p><p>在大原则上Turms在设计时遵循<code>能客户端自己实现的，Turms服务端就不实现，以实现最大的吞吐量也灵活业务实现。如果特性必须由服务端实现，且对吞吐量影响不大，则默认开启，否则默认关闭</code>，具体而言：</p><ul><li><p>可达性</p><ul><li>方案一：如果您希望实现几乎100%的消息必达，您可以开启<code>turms.service.message.sequence-id</code>下的<code>use-sequence-id-for-group-conversation</code>与<code>use-sequence-id-for-private-conversation</code>（默认配置下，均关闭），该机制会在每次生成消息记录时，向Redis请求一个会话级别的自增<code>sequence ID</code>，并将这个ID赋给当前消息记录上，客户端可以通过这个ID的自增性判断消息是否丢失</li><li>方案二（默认实现）：如果您不要求消息必须100%必达，则关闭上述配置，从而获得更大的消息推送吞吐量</li></ul></li><li><p>有序性</p><ul><li>顺序最终一致性 <ul><li>方案一：借助上述提到的自增<code>sequence ID</code>“顺便”实现消息的有序性</li><li>方案二：（默认实现）使用服务端时间保证消息顺序。提醒：不仅仅是消息需要使用系统时间，Turms服务端各个功能模块也重度使用系统时间，如基于Snowflake算法生成的ID</li></ul></li><li>接收顺序一致性：部分IM系统会通过延迟发送消息或客户端延迟展示消息，来尽可能避免“客户端先接收到在后发送的消息、再接收到在前发送的消息”，导致消息UI需要重排。但Turms暂未计划提供相关支持</li><li>因果一致性：（TODO）客户端发送消息时，可以携带一条<code>preMessageId</code>指示客户端UI上看到的上一条消息ID是什么，该记录对Turms自身没任何实际作用，但其他客户端可参考该值做上层的消息UI展示，以实现客户端之间看到的消息因果一致。</li></ul></li><li><p>重复性。Turms服务端在这方面只提供全局ID唯一的消息记录，消息的去重工作需要开发人员自行在客户端实现：如果您的应用需要实现100%的消息去重，则需要考虑落盘存储已接收的消息ID。如果您的应用只需要保证一个应用的生命周期内消息去重，那就只需要在内存中存储已接收的消息ID，每当服务端推送来新消息只需判断该ID的消息是否已处理过即可</p></li></ul><p>另外，下文会把一个业界常见但却通常非常失败的设计方案，即采用<code>需要服务端参与的消息确认机制</code>方案作为反面案例进行讲解。它用最高的成本实现了最差的“可达性”与“重复性”的效果，并且性能与拓展性也都极差。<strong>（TODO：尚未更新该部分文档）</strong></p><h3 id="消息确认机制-acknowledge" tabindex="-1"><a class="header-anchor" href="#消息确认机制-acknowledge" aria-hidden="true">#</a> 消息确认机制（Acknowledge）</h3><p>值得注意的是：</p><ol><li>Turms的消息确认机制并不需要Turms服务端的参与</li><li>消息确认机制与业务层面“消息已读”功能是完全独立的，二者没有关联关系。</li></ol><table><thead><tr><th></th><th>需要服务端参与的Ack机制</th><th>不需要服务端参与的Ack机制</th></tr></thead><tbody><tr><td>介绍</td><td>部分即时通讯架构设计中，会要求客户端在接受到消息后，间隔一定时间（如5秒、10秒等），向服务端发送消息确认请求（而不是一接受到消息就确认。一是为了提高确认处理效率，二是减少因网络延迟问题丢消息的概率）。<br>服务端记录每个会话最新的确认时间，以实现用户在对所有会话进行消息拉取时（如用户上线时），可以通过一个简单的请求去拉取确认时间至今的所有消息。</td><td>客户端本地存储每个会话的最后确认时间，客户端如果想获得任意其所属的会话消息，则向服务端发送对应的会话ID与确认时间，服务端会返回确认时间至今的所有消息。</td></tr><tr><td>优点</td><td>1. 客户端实现简单，无需在本地存储会话信息</td><td>1. 客户端可以自定义消息拉取范围。业务适用面更广，可以很轻松支持多端消息同步功能<br>2. 服务端不需要先查一次所有会话的确认时间，再根据Ack时间拉取消息，性能更优<br>3. 不需要客户端定时发送确认请求给服务端，能够完全省去大量确认操作带来的性能开销</td></tr><tr><td>缺点</td><td>1. 服务端需要先查一次所有会话的确认时间，再根据确认时间拉取消息，性能相对差<br>2. 对于受到的每一条消息，客户端都需要向服务端发送确认请求，然后服务端更新对应的消息状态，性能低下</td><td>1. 客户端发请求时，需要携带所有欲请求消息的会话ID与其对应的确认时间，请求体相对较大（但也对应了上述②的优点）<br>2. 需要开发者自行实现客户端本地数据库（如：Realm数据库。Turms未来可能会以拓展形式，帮助开发者实现本地存储功能）</td></tr></tbody></table><h3 id="关于消息的可达性" tabindex="-1"><a class="header-anchor" href="#关于消息的可达性" aria-hidden="true">#</a> 关于消息的可达性</h3><p>架构设计永远是平衡的艺术，盲目承诺消息100%必达只是一种销售的说辞。好比大部分互联网应用在分布式事务的技术实现上，只会采用性能更好的弱分布式事务，而非虽然更可靠但性能低下的强分布式事务。是否需要实现100%的消息必达还是根据业务场景而定（如在直播聊天室场景，不仅不要求消息必达，甚至还会要求服务端能主动根据负载情况，抛弃用户消息）。</p><p>实现消息100%必达的方案也比较简单，可以通过Redis实现一个会话级别的自增ID生成服务器，保证消息ID在一个会话内递增。客户端能通过ID的递增性自行判断是否有消息丢失，如果发现消息丢失，则发请求向服务端拿取指定消息即可。</p><p>Turms会同时支持上述的会话级消息自增ID实现来保证消息100%必达（TODO），同时也提供基于Snowflake算法的全局自增ID实现来提供最佳的吞吐量（代价就是消息不能保证100%必达）。</p><h3 id="关于离线推送的实现" tabindex="-1"><a class="header-anchor" href="#关于离线推送的实现" aria-hidden="true">#</a> 关于离线推送的实现</h3><p>对于在线用户，开发者可以通过notification属性来配置是否让服务端主动推送消息给在线用户（默认为true）。对于离线用户，离线推送的实现通常需要借助手机运营商提供的推送SDK，通过其通道进行离线推送。</p><p>但由于Turms本身不接入任何运营商，也没计划接入，因此您需要通过NotificationHandler插件来实现自定义的离线推送逻辑。该Handler提供一个handle函数，并接受消息信息、在线用户ID与离线用户ID这三个参数，您可以自行通过该函数调用厂商提供的推送SDK，来实现离线推送逻辑。</p><h3 id="特大群" tabindex="-1"><a class="header-anchor" href="#特大群" aria-hidden="true">#</a> 特大群</h3><p>特大群的实现其实并不难，只是它的需求与场景跟一般社交应用的很不一样，所以要有一套专门的策略来支持特大群。</p><p>策略（TODO）</p><ol><li>消息按照优先级发送</li><li>智能限制消息峰值，主动根据服务端状况与消息优先级丢消息</li><li>分桶（分小群）发消息</li><li>通常不需要消息漫游功能</li></ol>',64),c={},o=(0,d(3744).Z)(c,[["render",function(e,r){const d=(0,t.up)("OutboundLink");return(0,t.wg)(),(0,t.iD)(t.HY,null,[a,(0,t._)("p",null,[i,(0,t._)("a",l,[h,(0,t.Wm)(d)]),s]),n],64)}]])},3744:(e,r)=>{r.Z=(e,r)=>{for(const[d,t]of r)e[d]=t;return e}},4839:(e,r,d)=>{d.r(r),d.d(r,{data:()=>t});const t={key:"v-0addc666",path:"/for-developers/status-aware.html",title:"状态感知",lang:"en-US",frontmatter:{},excerpt:"",headers:[{level:2,title:"用户在线状态感知",slug:"用户在线状态感知",children:[{level:3,title:"具体原理（拓展知识）",slug:"具体原理-拓展知识",children:[]}]},{level:2,title:"业务模型状态感知",slug:"业务模型状态感知",children:[{level:3,title:"感知方式",slug:"感知方式",children:[]},{level:3,title:"客户端对用户行为感知的实时性与服务端延迟",slug:"客户端对用户行为感知的实时性与服务端延迟",children:[]}]},{level:2,title:"消息感知",slug:"消息感知",children:[{level:3,title:"读扩散与写扩散",slug:"读扩散与写扩散",children:[]},{level:3,title:"消息接收、消息更新与消息撤回",slug:"消息接收、消息更新与消息撤回",children:[]},{level:3,title:"关于消息的可达性、有序性与重复性",slug:"关于消息的可达性、有序性与重复性",children:[]},{level:3,title:"消息确认机制（Acknowledge）",slug:"消息确认机制-acknowledge",children:[]},{level:3,title:"关于消息的可达性",slug:"关于消息的可达性",children:[]},{level:3,title:"关于离线推送的实现",slug:"关于离线推送的实现",children:[]},{level:3,title:"特大群",slug:"特大群",children:[]}]}],filePathRelative:"for-developers/status-aware.md",git:{updatedTime:1639907278e3}}}}]);